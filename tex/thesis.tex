\documentclass[a4paper,onesided,12pt]{report}
\usepackage{styles/fbe_tez}
\usepackage[utf8x]{inputenc} % To use Unicode (e.g. Turkish) characters
\renewcommand{\labelenumi}{(\roman{enumi})}
\usepackage{amsmath, amsthm, amssymb}
 % Some extra symbols
\usepackage[bottom]{footmisc}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{longtable}
\graphicspath{{figures/}} % Graphics will be here

%\usepackage{xtab}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{algorithm}
\usepackage{algorithmic}
%\pagestyle{empty}
%\includeonly{introduction} % To only process the given file

\usepackage{tikz}
%\usepackage{xtab}
\newcommand{\R}{\mathbb{R}}
%\pagestyle{empty}
%\includeonly{introduction} % To only process the given file

%\usepackage{geometry}
\usepackage[short]{optidef}
\newcommand{\abs}[1]{\lvert #1 \rvert}
\usepackage{bbm}
\usepackage{url}

% COVER PAGE
\title{GRAPH LEARNING APPROACH FOR PREDICTING WIND POWER PRODUCTION IN AEGEAN REGION OF TURKEY}
\turkcebaslik{EGE BÖLGESİ RÜZGAR GÜCÜ ÜRETİM TAHMİNLEMESİNDE ÇİZGE ÖĞRENMESİ YAKLAŞIMI}
\degree{B.S., Industrial Engineering, Boğaziçi University, 2019\\
	B.S., Mathematics, Boğaziçi University, 2019}

\author{Mert Sarıkaya}
\program{Industrial Engineering}
\subyear{2022}

% APPROVED BY PAGE
\supervisor{Assist. Prof. Mustafa Gökçe Baydoğan}
%\cosuperi{Title and Name of Cosupervisor I}
%\cosuperii{Title and Name of Cosupervisor II}
\examineri{Prof. Necati Aras}
\examinerii{Assoc. Prof. Mehmet Gönen}

%\examineriv{}
%\examinerv{}
\dateofapproval{19.06.2022}

\begin{document}

\pagenumbering{roman}
\makemstitle % M.S. thesis
\makeapprovalpage

\begin{acknowledgements}

Above all, I would like to express my utmost gratitude to Assist. Prof. Mustafa Gökçe Baydoğan for his invaluable and continued guidance throughout this study.

Finally, I want to thank Scientific and Technological Research Council of Turkey (TUBITAK) for their financial support during my graduate study through BIDEB-2210A scholarship. This support was especially valuable in the early days of my graduate study.

\end{acknowledgements}

\begin{abstract}

In this research, 

\end{abstract}

\begin{ozet}

Bu araştırmada

\end{ozet}

\tableofcontents
\listoffigures
\listoftables

\begin{symbols}
% The title will be typeset as "LIST OF SYMBOLS".
%
% Use a separate \sym command for each symbols definition.
% First, Latin symbols in alphabetical order
\sym{$C$}{Total number of classes in the data set}
\sym{$d^{(i)}_{j}$}{Binary variable showing whether the instance $i$ is predicted as class $j$}
\sym{$\mathbb{J}$}{Set of all classes in the data set}
\sym{$pr^{(i)}$}{The profit the bank made from customer $i$}
% 1 EMPTY LINE BETWEEN LATIN AND GREEK SYMBOLS GROUP!!!
\sym{}{}
% Then Greek symbols in alphabetical order
\sym{$\epsilon$}{Margin between the score values corresponding to an instance}
\sym{$\theta^{(j)}_{k}$}{The coefficient of the class \emph{j} for the feature $k$}
\end{symbols}

\begin{abbreviations}
 % Abbreviations in alphabetical order
\sym{DNN}{Deep Neural Network}
\sym{ML}{Machine Learning}
\sym{NN}{Neural Network}
\sym{ReLU}{Rectified Linear Unit}

\end{abbreviations}

\chapter{INTRODUCTION}
\label{chapter:introduction}
\pagenumbering{arabic}

In the context of machine learning, classification algorithms are used to predict the class information of test data, given a set of training instances of size $N$ $\{(x^{(1)},y^{(1)}), \dots, (x^{(N)},y^{(N)})\}$, where $x^{(i)} \in \mathbb{R}^{K}$ is a feature vector and $y^{(i)} \in \{1, \dots, C\}$ represents its corresponding class information. Standard methods aim at learning a mapping function $f$ from input variables $x^{(i)}$ to discrete output variable $y^{(i)}$ that

However, these methods require more computational operations and demonstrate some downsides \cite{khan2015cost}. Undersampling brings about loss of information about the majority class and oversampling creates copies of minority class artificially which in turn may distort the data. In addition, studies show that prior training is only effective in binary classification case \cite{1549828}, and even have a negative effect in multiclass case \cite{Zhou}. 

cost-sensitive manner. A deep analysis of the previous studies that introduce costs during training is presented in Chapter \ref{sec:EDCSapproaches}.

Bengio \emph{et al.} \cite{bengio2020machine}; and also formulating a deep learning model as mixed-integer linear programming \cite{fischetti2017deep, anderson2020strong}.  CSLR bears the characteristics of both types of studies. Furthermore, we also empirically show that by providing 
\newpage
The rest of the thesis is organized as follows: Chapter \ref{sec:background} presents the backgrou are presented in Chapter \ref{sec:mathmodel}. Chapter \ref{sec:computationalexperiment} demonstrnts and on the real-world financial credit-scoring problem using realized financial returns. Finally, the conclusion of the thesis is drawn in Chapter \ref{sec:conclusion}.

\chapter{BACKGROUND}
\label{sec:background}

As a core method in machine learning, regression makes use of linear predictor with a set of parameters $\theta$ to predict the value of $y^{(i)}$ for the $i^{th}$ example $x^{(i)}$. Whereas linear regression uses a linear function $y = h_{\theta}(x) = \theta^{\top} x$ as hypothesis class, logistic regression feeds this linear predictor into a nonlinear function called \emph{logistic (sigmoid)} to generate the probability that a given example belongs to the "1" class versus the probability that it belongs to the "0" class. Hence, it builds a binary classification model on the training set $\{(x^{(1)}, y^{(1)}), \ldots, (x^{(N)}, y^{(N)}) \}$ of \emph{N} labeled examples, where the input features are $x^{(i)} \in \R^{K}$ and the labels take binary values, $y^{(i)} \in \{0,1\}$. It's hypothesis takes the form:
\begin{align}
h_\theta(x) = \frac{1}{1+\exp(-\theta^\top x)}.
\end{align}
\indent The probability values are then determined as $P( y= 1|x) = h_\theta(x)$ and $P( y= 0|x) = 1 - h_\theta(x)$. To build the best possible model, the performance of the hypothesis should be measured and optimized for parameters to get $\theta^{\star}$. For this purpose, logistic regression cost function can be written as:
\begin{equation}
{\label{eq:logisticcost}}
J(\theta) = - \left[ \sum_{i=1}^{N} \sum_{k=0}^{1} 1\left\{y^{(i)} = k\right\} \log P(y^{(i)} = k | x^{(i)} ; \theta) \right].    
\end{equation}

\begin{table}[thbp]
%\vskip\baselineskip 
\caption[Classification cost matrix for instance i]{Classification cost matrix for instance i.}
\begin{center}
\begin{tabular}{|c|c|c|} \hline
 \multirow{2}{*}{} & \textbf{Actual Positive}& \textbf{Actual Negative} \\
 &$y^{(i)} = 1$ &$y^{(i)} = 0$ \\\hline
\multirow{2}{*}{}\textbf{Predicted Positive} &\multirow{2}{*}{$C_{TP^{(i)}}$} &\multirow{2}{*}{$C_{FP^{(i)}}$} \\ 
         $d^{(i)} = 1$ & & \\\hline
\multirow{2}{*}{}\textbf{Predicted Negative} &\multirow{2}{*}{$C_{FN^{(i)}}$} &\multirow{2}{*}{$C_{TN^{(i)}}$} \\ 
         $d^{(i)} = 0$ & &  \\\hline
\end{tabular}
\label{table:edcscostmatrix}
\end{center}
\end{table}


\begin{table}[thbp]
    \caption{Returns matrix for example-dependent cost-sensitive learning.}
\begin{center}

    \begin{tabular}{|c|ccc|}\hline
        \textbf{Example}  & \textbf{Class 1} & $\cdots$ & \textbf{Class C}\\
      \hline
      $\textbf{1}$ &  $r^{(1)}_{1}$ & $\cdots$ & $r^{(1)}_{C}$  \\    
      $\vdots$ & & & \\
       $\textbf{i}$ &  $r^{(i)}_{1}$ & $\cdots$ & $r^{(i)}_{C}$  \\
             $\vdots$ & & & \\
       $\textbf{N}$ &  $r^{(N)}_{1}$ & $\cdots$ & $r^{(N)}_{C}$  \\\hline
    \end{tabular}
    \label{table:edcsmulticostmatrix}
\end{center}    
\end{table}  

\newpage
EDCS classification problem can be then reformulated such that $x^{(i)} \in \R^{K}$ is a feature vector and $r^{(i)} \in \R^{C}$ is a known vector of returns, together forming the training set $\{(x^{(1)},r^{(1)}), \dots, (x^{(N)},r^{(N)})\}$. Using multinomial logistic regression, the objective function takes then the following form with estimated probabilities:

\chapter{RELATED WORK}
\label{sec:literature}

In this chapter, we first show the previous studies on cost-sensitive learning that use cost information in the learning phase. Then, the contributions to the intersection of machine learning and optimization are introduced. 

\section{Section1}
\label{sec:Section1}

In literature, there have been several algorithms incorporating cost information during the learning process, but some of them are only applicable on the binary classification case whereas some others take the class-dependent costs into account, not the example-dependent ones.

\newpage

\section{Section2}
\label{sec:Section2}

Based on the presented templates, the proposed CSLR approach belongs to \emph{end to end learning}, where an ML model is trained to find a solution to an existing mixed-integer programming formulation. In addition, MIP-WI model can be considered to follow \emph{learning to configure algorithms} principle, demonstrating how providing an initial solution may reduce the total number of operations, hence expediting the optimization process.

\chapter{GRAPH LEARNING TIME SERIES MODEL}
\label{sec:graph} 

This chapter introduces our EDCS learning framework. The proposed method builds a general mathematical formulation as a mixed-integer programming model that maximizes the total returns in a given EDCS problem. After introducing the approach, the thesis will focus on how to make use of gradient descent as an approximation to the formulated MIP model.

\section{Graph1}
\label{sec:graph1}

Recall the reformulated EDCS classification problem where $x^{(i)} \in \R^{K}$ is a feature vector, $r^{(i)} \in \R^{C}$ is a vector of returns and $d^{(i)} \in \R^{C}$ denotes the decision maximum score value $\max_{j \in \mathbb{J}} p^{(i)}$. Then, the instance is assigned the class with the maximum score. To determine these values, a score function \emph{f} is defined as:

\begin{equation}
{\label{eq:MapFunction}}
  f \colon x^{(i)} \to p^{(i)}_{j}, \quad i \in \mathbb{S}, \quad j \in \mathbb{J}.
\end{equation}
\indent The scoring scheme is outlined in Equation \ref{eq:ScoreFunc} if a linear mapping function is used similar to the aforementioned logistic regression model, but without transforming the scores into probabilities
\noindent Thus, we achieve the following model:

\section{Graph2}
\label{sec:grapg2}

Neural networks can be customized to represent the proposed MIP model and are fast in finding feasible solutions using gradient descent based optimization. Despite requiring hyperparameter tuning, gradient descent and its variants are fairly easy and fast methods to tackle optimization problems.

Recall \emph{end to end learning}, where an ML model is trained to find a solution to an existing linear programming formulation. Accordingly, our aim is to reformulate EDCS mixed-integer linear programming model to make use of deep learning frameworks. To this end, here we introduce \emph{Cost-sensitive Logistic Regression} (CSLR), a nonlinear approximation of the MIP model for EDCS learning.

The idea of reformulating the MIP model as a neural network stems from the scoring mechanism described in Equation \ref{eq:ScoreFunc}, which is the same as the layer-wise operation in a neural network without an activation function, where the output is the weighted sum of the inputs: $p^{(i)}_{j} {= \theta^{(j)\top} x^{(i)}}, i \in \mathbb{S}, j \in \mathbb{J}$. Without adding bias term, the scores can be thus regenerated with a single layer perceptron with \emph{C} nodes in the output layer,  where the matrix $\theta$, $\theta \in \R^{C \times K}$, represents the weights. 

\indent As in the MIP model, the next step is comparing the scores and determining the class with the maximum score for labelling decisions. The approximation of the class assignment variable $d$ in MIP is achieved by a modification on the output layer. For instance, the outputs can be converted into probabilities using \emph{softmax} activation function. Multiplication of the probabilities with the returns leads to the objective of maximizing the expected returns. Instead, the logits $p_{j}$ of softmax, the vector of raw predictions the last layer generates, is multiplied with a large number $M$, so that the outputs of this function will be pushed towards 1 and 0, where the largest score value approximates to 1, and the others to 0 regardless of the number of classes. At the end of the layer, the predictions $d_{j}$ take values as following:
\begin{equation}
{\label{eq:Softmax}}
  d_{j} = \frac{\exp(M p_{j} )}{ \sum_{t=1}^{C}{\exp(M p_{t})}}, \quad  j \in \mathbb{J}.
\end{equation}

Unlike the proposed linear programming model, this approach is applicable from small to large scale of problems. Furthermore, having an incumbent solution very fast would bring about the idea of accelerating the main MIP formulation. As mentioned, a linear programming solver can be very slow to find an initial feasible solution giving a large-scale problem. Following the principle of \emph{learning to configure algorithms} \cite{bengio2020machine}, providing the solutions from CSLR to the MIP model as an initial solution can help in terms of increasing the speed to maximize the total return in the MIP approach. Providing a tighter lower bound for the objective can make it possible for tree-based branch-and-bound method to conduct better pruning within a given time limit, hence increasing the efficiency in MIP. To this end, we propose an additional approach called mixed-integer programming with the initial solution (MIP-WI), which takes the updated parameters $\theta$ from the perceptron and feeds them into the MIP algorithm as an initial solution.

The main advantage of formulating the proposed optimization model in neural network architecture is its speed and control over the model. In addition, one can add more layers to increase model complexity and tackle complex EDCS learning problems and obtain better results. For the sake of simplicity and the full representation of the proposed MIP approach, single layer perceptron is preferred in this work.

\chapter{COMPUTATIONAL EXPERIMENTS}
\label{sec:computationalexperiment}

First, the optimization performances of our proposals CSLR, MIP-WI and MIP are compared. To formulate EDCS model as a single layer perceptron, \emph{PyTorch} environment is preferred where each model is run for 1,000 epochs using Adam optimizer \cite{kingma2014adam} and makes use of batch gradient descent. Then, the last updated weight values of the gradient descent based 

The test performances of the proposed approaches are first analyzed when compared with some state-of-the-art machine learning models: Logistic Regression, Decision Trees and boosting trees, namely Xgboost \cite{Chen_2016}. These models are trained with default parameters in their respective libraries \emph{scikit-learn} \cite{DBLP:journals/corr/abs-1201-0490} and \emph{xgboost} \cite{Chen_2016}, except that the maximum depth of tree-based algorithms is fixed at $8$ to prevent building very deep trees.

\begin{equation}
{\label{eq:ScoreFunction}}
  \emph{Normalized Return} { = \dfrac{
  \sum_{i \in \mathbb{S}}r^{(i)\top} d^{(i)}}
  {\sum_{i \in \mathbb{S}} \max_{j \in \mathbb{J}} r^{(i)}}
  }. \quad 
\end{equation}
\indent The experiments are held using Python3.6.5 and the numerical results have been obtained on MSI MS-7A93 with an i9-7900X CPU and 32GB RAM and can be reproducible. The codes and the data sets can be found on the github page of the author \cite{tarkan2021}.

\section{Experiments on Real-Life Data - Credit Scoring}
\label{sec:experimentsreal}

In this experiment, for the linear models, Logistic Regression and our proposed EDCS approaches, the data is centered and scaled between zero and one as the range of the values of features varies among each other drastically. When working with gradient descent, scaling makes optimization converge \cite{LeCunBOM12}. It also increases the interpretability of the coefficients and makes sensitivity analysis possible for the relative changes of the coefficients of covariates on the score values. 

\subsection{Results}
\label{sec:bankresults}

\subsection{Comparison with other EDCS Learning Models}
\label{sec:bankresultscostcla}

\section{Discussion}
\label{sec:discussion}

classifications and misclassifications.

\chapter{CONCLUSION}
\label{sec:conclusion}

In this thesis, we illustrate how EDCS learning problems can be successfully addressed if example-dependent costs/returns

% \nocite{NewEntry1,NewEntry2,NewEntry3,NewEntry4,NewEntry5,
% NewEntry6,
% NewEntry7,NewEntry8,NewEntry9,NewEntry10,NewEntry11,NewEntry12}


%\cite{*}
\bibliographystyle{styles/fbe_tez_v11}
\bibliography{references}

%\appendix
%\chapter[AN APPENDIX TITLE THAT IS LONG AND THEREFORE\\
	%\hspace*{2.95cm} NEEDS MANUAL ADJUSTMENT IN LATEX CODE TO FIT\\
	%\hspace*{2.95cm} PROPERLY IN TABLE OF CONTENTS]{AN APPENDIX TITLE THAT IS LONG AND THEREFORE NEEDS MANUAL ADJUSTMENT IN LATEX CODE TO FIT PROPERLY IN TABLE OF CONTENTS}
%The appendices start here. After references section.

\end{document}